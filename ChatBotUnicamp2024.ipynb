{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhTJpsOedT+ehVqt6Sxe2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoMKlGui/ProjetoNeuralMind/blob/master/ChatBotUnicamp2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baixando todas as dependências do projeto"
      ],
      "metadata": {
        "id": "MN-3npRGrFna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOxWi7oi_ob1"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab]\n",
        "wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.04.tar.gz\n",
        "tar -xvf xpdf-tools-linux-4.04.tar.gz && sudo cp xpdf-tools-linux-4.04/bin64/pdftotext /usr/local/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definindo as funções para a criação do retriever e a pipeline generativa"
      ],
      "metadata": {
        "id": "9ZjswLBmrLNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser, BM25Retriever\n",
        "from haystack.pipelines import Pipeline\n",
        "\n",
        "def init_prompt():\n",
        "    prompt_template = PromptTemplate(\n",
        "        prompt=\"\"\"Responda a questão de forma precisa baseado apenas no documento que lhe foi passado. Caso a informação não esteja no documento, diga que uma resposta não pode ser gerada apenas com as informações dadas.\n",
        "        Documento:{join(documents)}\n",
        "        Pergunta:{query}\n",
        "        stop_words=[\"Observação:\"],\n",
        "        Resposta:\n",
        "        \"\"\",\n",
        "        output_parser=AnswerParser()\n",
        "    )\n",
        "\n",
        "    return prompt_template\n",
        "\n",
        "\n",
        "def init_prompt_node(api_key_openai: str, model_name):\n",
        "    prompt_node = PromptNode(\n",
        "        model_name_or_path = model_name,\n",
        "        api_key=api_key_openai,\n",
        "        default_prompt_template=init_prompt()\n",
        "    )\n",
        "\n",
        "    return prompt_node\n",
        "\n",
        "def init_retriever(document_store):\n",
        "    retriever = BM25Retriever(\n",
        "        document_store = document_store\n",
        "    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "\n",
        "def init_generative_pipeline(document_store, open_ai_key):\n",
        "    generative_pipeline = Pipeline()\n",
        "    generative_pipeline.add_node(\n",
        "        component = init_retriever(document_store),\n",
        "        name = \"retriever\",\n",
        "        inputs=[\"Query\"]\n",
        "    )\n",
        "    generative_pipeline.add_node(\n",
        "        component=init_prompt_node(open_ai_key, \"text-davinci-003\"),\n",
        "        name = \"prompt_node\",\n",
        "        inputs = [\"retriever\"]\n",
        "    )\n",
        "\n",
        "    return generative_pipeline"
      ],
      "metadata": {
        "id": "nyfR1MSiAAs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definindo a pipeline para transformação do pdf em um document do Haystack"
      ],
      "metadata": {
        "id": "rWykXzqMrSw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline para extrair infos das páginas web do regimento do vestibular unicamp 2024\n",
        "'''\n",
        "Installing external dependencies\n",
        "\n",
        "Todas essas dependências estão presentes em \"requirements.txt\"\n",
        "\n",
        "'''\n",
        "\n",
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "from haystack.nodes import PDFToTextConverter\n",
        "\n",
        "def init_document_store(document):\n",
        "    doc_store = InMemoryDocumentStore(use_bm25 = True)\n",
        "    doc_store.write_documents(document)\n",
        "\n",
        "    return doc_store\n",
        "\n",
        "\n",
        "def convert_pdf_to_text(path: str):\n",
        "    \"\"\"\n",
        "    returns a document\n",
        "    \"\"\"\n",
        "    conversor = PDFToTextConverter()\n",
        "\n",
        "    documento_text = conversor.convert(file_path = path)\n",
        "\n",
        "    return documento_text\n"
      ],
      "metadata": {
        "id": "avXq73rAAHYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agora, criando as funções para que o mecanismo de chat-bot funcione. O coração do código!"
      ],
      "metadata": {
        "id": "3i5p1sw2reB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "from haystack.agents import Tool\n",
        "from haystack.agents import AgentStep, Agent\n",
        "from haystack.agents.base import Agent, ToolsManager\n",
        "from haystack.nodes import PromptNode\n",
        "from haystack.agents.memory import ConversationSummaryMemory\n",
        "\n",
        "prompt_agente = \"\"\"\n",
        "    Nesta conversa, o usuário humano interage com uma Inteligência Artificial, que será o agente. O usuário faz perguntas e o agente passa por diversos processamentos para retornar uma resposta bem informada ao usuário.\n",
        "    O agente deve utilizar as ferramentas disponíveis para achar informações atualizadas e a resposta deve ser baseada somente na saída das ferramentas.\n",
        "    O agente deve ignorar todo seu conhecimento prévio para responder a pergunta, utilizando somente as ferramentas passadas.\n",
        "    O agente tem acesso a estas ferramentas:\n",
        "    {tool_names_with_descriptions}\n",
        "\n",
        "    A seguir, tem a conversa prévia entre humano e IA:\n",
        "    {memory}\n",
        "\n",
        "    A resposta do agente IA deve começar com algum destes:\n",
        "    Pensamento: [O processo de raciocínio do agente]\n",
        "    Ferramenta: [Nome das ferramentas] (em uma nova linha) Entrada da Ferramenta: [entrada como uma pergunta para a ferramenta selecionada SEM aspas e em uma nova linha] (Estes devem ser sempre fornecidos juntos e em linhas separadas.)\n",
        "    Observação: [Resultado da ferramenta]\n",
        "    Resposta Final: [Resposta final para a pergunta do humano]\n",
        "    Ao selecionar uma ferramenta, o Agente deve fornecer o par \"Ferramenta:\" e \"Entrada da ferramenta:\" na mesma resposta, mas em linhas separadas.\n",
        "    O Agente não deve solicitar informações, esclarecimentos ou contexto adicionais ao usuário humano.\n",
        "    Se o Agente não conseguir encontrar uma resposta específica após esgotar as ferramentas e abordagens disponíveis, ele responderá com a Resposta Final: Inconclusivo, sinto muito.\n",
        "\n",
        "    Pergunta: {query}\n",
        "    Pensamento:\n",
        "    {transcript}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def init_search_tool(gen_pipeline):\n",
        "    search_tool = Tool(\n",
        "        name=\"busca_vest_unicamp_2024\",\n",
        "        pipeline_or_node=gen_pipeline,\n",
        "        description=\"útil quando você precisa responder perguntas sobre o vestibular 2024 da unicamp\",\n",
        "        output_variable=\"respostas\",\n",
        "    )\n",
        "\n",
        "    return search_tool\n",
        "\n",
        "def resolucao(query, agent, agent_step):\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"tool_names_with_descriptions\": agent.tm.get_tool_names_with_descriptions(),\n",
        "        \"transcript\": agent_step.transcript,\n",
        "        \"memory\": agent.memory.load()\n",
        "    }\n",
        "\n",
        "def run_conversation(api_key, pdf): #essa função irá realizar a conversão e junção dos diferentes pedidos para gerar uma resposta do chat gpt\n",
        "    document = convert_pdf_to_text(pdf)\n",
        "    doc_store = init_document_store(document)\n",
        "    gen_pipeline = init_generative_pipeline(doc_store, api_key)\n",
        "    search_tool = init_search_tool(gen_pipeline)\n",
        "    agent_prompt_node = PromptNode(\"gpt-3.5-turbo\", api_key = api_key, max_length=128)\n",
        "    memory_prompt_node = PromptNode(\n",
        "    \"philschmid/bart-large-cnn-samsum\", max_length=128, model_kwargs={\"task_name\": \"text2text-generation\"}\n",
        "    )\n",
        "    memory = ConversationSummaryMemory(memory_prompt_node, prompt_template=\"{chat_transcript}\")\n",
        "\n",
        "    conversational_agent = Agent(\n",
        "        agent_prompt_node,\n",
        "        prompt_template=prompt_agente,\n",
        "        prompt_parameters_resolver=resolucao,\n",
        "        memory=memory,\n",
        "        tools_manager=ToolsManager([init_search_tool(gen_pipeline)]),\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        pergunta = input(\"Faça uma pergunta sobre o vestibular unicamp 2024!\")\n",
        "        response = conversational_agent.run(pergunta)\n",
        "        print(response)\n"
      ],
      "metadata": {
        "id": "-K-Ce47PAKFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agora está tudo pronto!\n",
        "Basta você fazer upload do pdf do vestibular 2024 no colab, colar o caminho dele em path e sua chave da Open AI"
      ],
      "metadata": {
        "id": "2-NOm3BHq3c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = r'coloque aqui o path'\n",
        "open_ai_key = \"colque aqui sua chave api da open ai\"\n",
        "\n",
        "\n",
        "run_conversation(open_ai_key, path)"
      ],
      "metadata": {
        "id": "gHL-s4anAsDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3XrZb8iBIC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}